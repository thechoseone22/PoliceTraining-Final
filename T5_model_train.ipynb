{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5907eca-7f75-4867-ae45-48b69908ca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\jasmi\\appdata\\roaming\\python\\python311\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c215026a-6c67-4054-bb43-e10c1b046760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jasmi\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFT5ForConditionalGeneration, T5Tokenizer\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3993be6-355d-4d33-a784-6bca8a382ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Default title text\n",
    "data = pd.read_csv(r\"C:\\Users\\jasmi\\Projects\\Final Project\\PoliceTraining-Final\\training_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd5279e-b4e2-43e0-a9af-91b3a72f19f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data[\"Bodycam Transcript\"].tolist()\n",
    "labels = data[\"Success/Failure\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a134e223-e7b6-438b-b2db-de38227208b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_mask = data[\"Success/Failure\"].isin([0, 1])\n",
    "inputs = data[valid_mask][\"Bodycam Transcript\"].tolist()\n",
    "labels = data[valid_mask][\"Success/Failure\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "320b9642-1b9e-45c7-bb7e-2be164060141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    inputs, labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2368a937-c9ce-49fa-bca1-aa0e6d416614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Tokenize training and validation data\n",
    "max_length = 512  # Adjust based on model or dataset requirements\n",
    "train_encodings = tokenizer(\n",
    "    train_texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"tf\"\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    val_texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"tf\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eecfe56-002b-4155-b5be-63e34f75da5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_right(labels):\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    decoder_input_ids = tf.concat(\n",
    "        [tf.fill([tf.shape(labels)[0], 1], pad_token_id), labels[:, :-1]],\n",
    "        axis=-1,\n",
    "    )\n",
    "    return decoder_input_ids\n",
    "\n",
    "train_decoder_input_ids = shift_right(train_encodings[\"input_ids\"])\n",
    "val_decoder_input_ids = shift_right(val_encodings[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1bc0c1d-2350-4a36-9824-dc7aa367bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    \"input_ids\": train_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": train_encodings[\"attention_mask\"],\n",
    "    \"labels\": train_decoder_input_ids\n",
    "}).shuffle(500).batch(8)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    \"input_ids\": val_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": val_encodings[\"attention_mask\"],\n",
    "    \"labels\": val_decoder_input_ids\n",
    "}).batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2fa9daf-92bc-4428-ace8-72111519f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a63f9650-05f6-4761-af11-ac1daf92e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jasmi\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), loss=model.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ade59ec-7abf-4bb9-a406-b00c1d0bd956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f32db5-20c2-47bd-bcca-73cddccb8daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, validation_data=val_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba658f16-899f-4870-9bdc-a04ad41254e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(r\"C:\\Users\\jasmi\\Projects\\Final Project\\PoliceTraining-Final\\trained_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e5ced-a545-4f1d-9c9f-016cc9504e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Hugging Face model\n",
    "model.save_pretrained(r\"C:\\Users\\jasmi\\Projects\\Final Project\\PoliceTraining-Final\\trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cb9f80c-f107-4f9b-b00e-28b8eb4e08b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at C:\\Users\\jasmi\\Projects\\Final Project\\PoliceTraining-Final\\trained_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tft5_for_conditional_generation_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " shared (Embedding)          multiple                  16449536  \n",
      "                                                                 \n",
      " encoder (TFT5MainLayer)     multiple                  35330816  \n",
      "                                                                 \n",
      " decoder (TFT5MainLayer)     multiple                  41625344  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60506624 (230.81 MB)\n",
      "Trainable params: 60506624 (230.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the Hugging Face model\n",
    "loaded_model = TFT5ForConditionalGeneration.from_pretrained(r\"C:\\Users\\jasmi\\Projects\\Final Project\\PoliceTraining-Final\\trained_model\")\n",
    "\n",
    "# Verify the loaded model\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37379d46-f0a1-48a3-b8ac-b84d8dfb1a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce batch size for the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    \"input_ids\": val_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": val_encodings[\"attention_mask\"],\n",
    "    \"labels\": val_decoder_input_ids\n",
    "}).batch(8)  # Reduce from 8 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a741907-058a-460b-83fe-60c6cbf8d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_encodings = tokenizer(\n",
    "    val_texts,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,  # Truncate to fit max_length\n",
    "    max_length=512,   # Ensure this matches your model's max input length\n",
    "    return_tensors=\"tf\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10286175-27e7-4388-ac9f-fa36d7176059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 199s 2s/step\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__ConcatV2_N_100_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[200,512,32128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Or '/TPU:0' if using TPU\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(val_dataset)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_tf_utils.py:1249\u001b[0m, in \u001b[0;36mTFPreTrainedModel.predict\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mpredict)\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1248\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\ops.py:6002\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   6001\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 6002\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__ConcatV2_N_100_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[200,512,32128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):  # Or '/TPU:0' if using TPU\n",
    "    predictions = model.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e02bb20-3bc4-4392-9095-0e0013db3fdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# predictions = model.predict(val_dataset)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# extract ze predicted labels from logits\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m predicted_logits \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m      5\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39margmax(predicted_logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# predictions = model.predict(val_dataset)\n",
    "\n",
    "# extract ze predicted labels from logits\n",
    "predicted_logits = predictions.logits\n",
    "predicted_labels = tf.argmax(predicted_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "611965d1-8a11-47e9-bf2b-31920777f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten true_labels to compare\n",
    "true_labels = []\n",
    "for batch in val_dataset:\n",
    "    true_labels.extend(batch[\"labels\"].numpy().flatten())\n",
    "\n",
    "# change (don't ask, tell) true_labels to NumPy \n",
    "true_labels = np.array(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15bdd09f-9051-4c9a-a28f-98edec502269",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m valid_indices \u001b[38;5;241m=\u001b[39m true_labels \u001b[38;5;241m!=\u001b[39m pad_token_id\n\u001b[0;32m      5\u001b[0m filtered_true_labels \u001b[38;5;241m=\u001b[39m true_labels[valid_indices]\n\u001b[1;32m----> 6\u001b[0m filtered_predicted_labels \u001b[38;5;241m=\u001b[39m predicted_labels\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()[valid_indices]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predicted_labels' is not defined"
     ]
    }
   ],
   "source": [
    "# filter padding tokens get em out\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "valid_indices = true_labels != pad_token_id\n",
    "filtered_true_labels = true_labels[valid_indices]\n",
    "filtered_predicted_labels = predicted_labels.numpy().flatten()[valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb69bdda-5a66-4574-bee3-617fc8af49cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy = accuracy_score(filtered_true_labels, filtered_predicted_labels)\n",
    "\n",
    "# Precision, Recall, and F1 Score\n",
    "precision = precision_score(filtered_true_labels, filtered_predicted_labels, average=\"binary\")\n",
    "recall = recall_score(filtered_true_labels, filtered_predicted_labels, average=\"binary\")\n",
    "f1 = f1_score(filtered_true_labels, filtered_predicted_labels, average=\"binary\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(filtered_true_labels, filtered_predicted_labels)\n",
    "\n",
    "# Display Results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725faf3-199f-4400-bacf-e8980ec42745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the confusion\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Success\", \"Failure\"], yticklabels=[\"Success\", \"Failure\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b7915-8eaa-4a15-bba6-8da83fa62e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "#     df[\"Bodycam Transcript\"], \n",
    "#     df[\"Success/Failure\"],     \n",
    "#     test_size=0.2,            \n",
    "#     random_state=42            \n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# train_labels = train_labels.to_numpy().reshape(-1, 1)\n",
    "# val_labels = val_labels.to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b080acce-947c-4941-80ac-592c88702466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the data\n",
    "# train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "#     texts, labels, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# # Tokenize validation data\n",
    "# val_encodings = tokenizer(\n",
    "#     val_texts,\n",
    "#     padding=\"max_length\",\n",
    "#     truncation=True,\n",
    "#     max_length=max_length,\n",
    "#     return_tensors=\"tf\"\n",
    "# )\n",
    "\n",
    "# # Shift decoder input IDs for validation\n",
    "# val_decoder_input_ids = shift_right(val_encodings[\"input_ids\"])\n",
    "\n",
    "# # Create TensorFlow datasets\n",
    "# batch_size = 8  # Adjust batch size as needed\n",
    "\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "#     \"input_ids\": train_encodings[\"input_ids\"],\n",
    "#     \"attention_mask\": train_encodings[\"attention_mask\"],\n",
    "#     \"labels\": train_decoder_input_ids\n",
    "# }).shuffle(500).batch(batch_size)\n",
    "\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "#     \"input_ids\": val_encodings[\"input_ids\"],\n",
    "#     \"attention_mask\": val_encodings[\"attention_mask\"],\n",
    "#     \"labels\": val_decoder_input_ids\n",
    "# }).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7727ae06-3917-4d50-be01-90d6fab60455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels_flat = train_labels.ravel()  \n",
    "# classes = np.array([0, 1])  \n",
    "# class_weights = compute_class_weight(\"balanced\", classes=classes, y=train_labels_flat)\n",
    "# class_weights_dict = {0: class_weights[0], 1: class_weights[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c96a107-08c5-4e68-87e3-08ac2d3f7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_weights = []\n",
    "# for label in train_labels_flat:  \n",
    "#     weight = class_weights_dict[int(label)]  \n",
    "#     sample_weights.append(weight)\n",
    "#     print(f\"Label: {label}, Assigned Weight: {weight}\")\n",
    "\n",
    "# # convert np.array\n",
    "# sample_weights = np.array(sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60268c40-c96e-4abc-94a5-a4d800376848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_decoder_input_ids = shift_right(train_encodings[\"input_ids\"])\n",
    "# val_decoder_input_ids = shift_right(val_encodings[\"input_ids\"])\n",
    "\n",
    "# # create dataset\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "#     \"input_ids\": train_encodings[\"input_ids\"],\n",
    "#     \"attention_mask\": train_encodings[\"attention_mask\"],\n",
    "#     \"labels\": train_decoder_input_ids\n",
    "# }).shuffle(500).batch(8, drop_remainder=True) #shuffle 1000 or 500 / / batch 4,8 or 16\n",
    "\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "#     \"input_ids\": val_encodings[\"input_ids\"],\n",
    "#     \"attention_mask\": val_encodings[\"attention_mask\"],\n",
    "#     \"labels\": val_decoder_input_ids\n",
    "# }).batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4635487f-0a99-4800-87ce-4f854dd41353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = tf.data.Dataset.from_tensor_slices((\n",
    "#     {\n",
    "#         \"input_ids\": input_encodings[\"input_ids\"],\n",
    "#         \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "#         \"decoder_input_ids\": label_encodings[\"input_ids\"]  \n",
    "#     },\n",
    "#     label_encodings[\"input_ids\"]  \n",
    "# ))\n",
    "# dataset = dataset.shuffle(1000).batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e15031-57d2-494d-8c54-0158fc8ff862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_loss_function(labels, logits):\n",
    "#     loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "#         from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
    "#     )\n",
    "#     tf.print(\"Labels in Custom Loss Function:\", labels)\n",
    "#     mask = tf.cast(labels != 0, dtype=tf.float32)  \n",
    "#     loss = loss_fn(labels, logits)\n",
    "#     loss *= mask\n",
    "#     weights = tf.gather([class_weights_dict[0], class_weights_dict[1]], labels)\n",
    "#     loss *= weights  \n",
    "#     return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5e0c14-9e64-4a97-9281-7fe8a5169d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_loss_function(labels, logits):\n",
    "#     # bug it\n",
    "#     tf.print(\"Labels Shape (before reshape):\", tf.shape(labels))\n",
    "#     tf.print(\"Logits Shape:\", tf.shape(logits))\n",
    "\n",
    "    \n",
    "#     labels = tf.reshape(labels, [-1])  \n",
    "#     logits = tf.reshape(logits, [-1, tf.shape(logits)[-1]])  \n",
    "\n",
    "    \n",
    "#     tf.print(\"Labels Shape (after reshape):\", tf.shape(labels))\n",
    "#     tf.print(\"Logits Shape (after reshape):\", tf.shape(logits))\n",
    "\n",
    "    \n",
    "#     loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#     loss = loss_fn(labels, logits)\n",
    "\n",
    "#     return tf.reduce_mean(loss)  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3073a53-ab7e-447c-9826-4565714d35a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function ignoring PAD tokens\n",
    "def custom_loss_function(labels, logits):\n",
    "    mask = tf.not_equal(labels, tokenizer.pad_token_id)  # Mask out padding tokens\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
    "    loss = loss_fn(labels, logits)\n",
    "    loss = tf.reduce_mean(loss * tf.cast(mask, tf.float32))  # Apply mask\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a87a8f-23e7-4137-9d8e-14b96e0bdd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=custom_loss_function) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920eadc0-f4ca-43a8-8940-10c6b6da9ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class Weights Mapping:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e7192e-23da-45af-a9bf-1225f964f8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique Train Labels:\", set(train_labels.flatten().tolist()))\n",
    "print(\"Unique Validation Labels:\", set(val_labels.flatten().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe07f8f-f41b-489f-a7a6-e3e19e50613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2407f599-754d-494c-84df-2ee1397d4c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1c4098-5e7d-4dc5-8fa8-b7e644da3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "#     df[\"Bodycam Transcript\"],  \n",
    "#     df[\"Success/Failure\"],     \n",
    "#     test_size=0.2,             \n",
    "#     random_state=42\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4189e14-b509-4c06-a3cd-261a99afdb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d720fd0-3ab8-4a4f-8ab9-58791c2c90d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_encodings = tokenizer(\n",
    "    list(val_texts), truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"tf\"\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(val_texts)):\n",
    "    input_ids = val_encodings[\"input_ids\"][i:i+1]\n",
    "    attention_mask = val_encodings[\"attention_mask\"][i:i+1]\n",
    "    output = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=10)\n",
    "    pred = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    predictions.append(pred.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde5e171-75b7-40d4-865a-b0c9bba3978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = [1 if pred == \"success\" else 0 for pred in predictions]\n",
    "\n",
    "true_labels = list(val_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a52dd25-7a5e-48ae-97a0-1a523adba105",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec37f25b-5d00-4d85-b8ca-9b97feb6db68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels, target_names=[\"Failure\", \"Success\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cfbfa2-418e-41f9-b3ce-03291729e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_distribution = train_labels.value_counts()\n",
    "val_class_distribution = val_labels.value_counts()\n",
    "\n",
    "print(\"Training Class Distribution:\")\n",
    "print(train_class_distribution)\n",
    "\n",
    "print(\"\\nValidation Class Distribution:\")\n",
    "print(val_class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad6e06d-9a20-451b-8e81-e2fa916ebd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f59d8b-4f65-4ed3-8b2c-0a8734dfaefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = set(STOPWORDS)\n",
    "custom_stopwords.update([\"officer\", \"miller\", \"subject\", \"sound\", \"october\", \"time\", \"and\", \"john\", \"doe\", \"sir\", \"I\", \"Audio\", \"Bodycam\", \"Transcript\"])\n",
    "\n",
    "text_data = \" \".join(val_texts)\n",
    "wordcloud = WordCloud(\n",
    "    width=800, height=400, background_color=\"white\", stopwords=custom_stopwords, colormap=\"viridis\"\n",
    ").generate(text_data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud of Bodycam Transcripts (Excluding Common Words)\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259e525-9396-425f-a2cf-ce2533a2dd62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
